{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nbimporter\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "from keras import backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D , InputSpec ,InputSpec\n",
    "from keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras.models import load_model\n",
    "from itertools import permutations \n",
    "import keras\n",
    "import ast\n",
    "from keras.utils import np_utils\n",
    "to_one_hot = np_utils.to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"vagalume.train.csv\")\n",
    "df_dev = pd.read_csv(\"vagalume.dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(\"\\n\",\" \",text)\n",
    "    text = re.sub(\"[,|!|\\?|\\.]\",\" \",text)\n",
    "    text = re.sub(\" +\",\" \",text)\n",
    "    \n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def get_X_Y(dataframe):\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for row,line in dataframe.iterrows():\n",
    "        \n",
    "        clean_title = clean_text(line['music_title'])\n",
    "        clean_lyric = clean_text(line['music_lyric'])\n",
    "        X.append((clean_title+\" \"+clean_lyric).split(\" \") )\n",
    "        Y.append(line['genre'])\n",
    "    \n",
    "    return X,Y\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train = get_X_Y(df_train)\n",
    "X_dev,Y_dev = get_X_Y(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words len: 996773\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "words_data_set = set([w.lower() for sentece in X_train for w in sentece ])\n",
    "model_word2vec = KeyedVectors.load_word2vec_format(\"word_embeddings/wang2vec/cbow_s100.txt\", unicode_errors=\"ignore\")\n",
    "words_word2vec = set(model_word2vec.vocab.keys())\n",
    "\n",
    "words = list( words_data_set.union(words_word2vec) )\n",
    "n_words = len(words)\n",
    "print(\"words len:\",n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags len: 14\n"
     ]
    }
   ],
   "source": [
    "tags_lyric = list(set( [tag for tag in (Y_dev+Y_train) ]  )  )\n",
    "n_tags_lyric = len(tags_lyric)\n",
    "print(\"tags len:\",n_tags_lyric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 290\n",
    "max_len_char = 20\n",
    "\n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1\n",
    "word2idx[\"PAD\"] = 0\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "tag2idx = {t: i + 1 for i, t in enumerate(tags_lyric)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_pad(senteces_param):\n",
    "    \n",
    "    word_pad = []\n",
    "    for s in senteces_param:\n",
    "        sentece_pad = []\n",
    "        for w in s:\n",
    "            if w.lower() in word2idx:\n",
    "                sentece_pad.append( word2idx[w.lower()])\n",
    "            else:\n",
    "                sentece_pad.append(1)\n",
    "        word_pad.append(sentece_pad)\n",
    "    \n",
    "    word_pad = pad_sequences(maxlen=max_len, sequences=word_pad, value=word2idx[\"PAD\"], padding='post', truncating='post')\n",
    "    \n",
    "    return word_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_word_tr = word_pad(X_train)\n",
    "X_word_dv = word_pad(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_pad(y_param):\n",
    "    return [tag2idx[tag] for tag in y_param]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_pad(Y_train)\n",
    "y_test = y_pad(Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Embedding Matrix\n",
    "embedding_matrix = np.random.random((n_words + 2, EMBEDDING_DIM))\n",
    "for word, i in word2idx.items():\n",
    "    if(word in model_word2vec):\n",
    "        embedding_matrix[i] = model_word2vec[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 290)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 290, 100)          99677500  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 290, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 512)               731136    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15)                7695      \n",
      "=================================================================\n",
      "Total params: 100,416,331\n",
      "Trainable params: 100,416,331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_layers=256\n",
    "\n",
    "#Model\n",
    "# input and embedding for words\n",
    "word_in = Input(shape=(max_len,))\n",
    "emb_word = Embedding(n_words + 2, EMBEDDING_DIM,\n",
    "                     weights=[embedding_matrix],input_length=max_len, mask_zero=True)(word_in)\n",
    "\n",
    "#BLSTM\n",
    "x = SpatialDropout1D(0.3)(emb_word)\n",
    "lstm = Bidirectional(LSTM(units=hidden_layers, return_sequences=False,\n",
    "                               recurrent_dropout=0.6))(x)\n",
    "\n",
    "out = Dense(n_tags_lyric+1, activation=\"softmax\")(lstm)\n",
    "\n",
    "model = Model(word_in, out)\n",
    "\n",
    "#Compile\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96857 samples, validate on 96857 samples\n",
      "Epoch 1/5\n",
      "96832/96857 [============================>.] - ETA: 1s - loss: 1.6572 - acc: 0.4815"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_word_tr, y_train,validation_data=( X_word_dv, y_test ), epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
